{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028911fa-cc9d-4b03-a5a1-d869c2fcf966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496fbfdd-6604-4046-be05-23207048e017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727dc817-07f7-4e4b-9991-13b6322f39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e5991c-8608-4caa-80d3-75a971506350",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd8239db-8fd1-4a0d-9139-4989228a158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c818b7-efe5-4454-875a-0b53da0e03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "        \"Lexi-Llama-3-8B-Uncensored_F16.gguf\", \"Lexi-Llama-3-8B-Uncensored_Q4_K_M.gguf\",\n",
    "        \"Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf\", \"Lexi-Llama-3-8B-Uncensored_Q8_0.gguf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f83dce-e3a2-402e-bb34-8cf1e2408a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45a8a537f924800826afcd0d4e98a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf:   0%|          | 0.00/5.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MIKARKON\\.cache\\huggingface\\hub\\models--Orenguteng--Llama-3-8B-Lexi-Uncensored-GGUF\\snapshots\\55cb207db4f777bdf8836d0ac5986c661280822b\\Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "filename=filenames[2];\n",
    "downloaded_model_path = hf_hub_download(\n",
    "                    repo_id=model_id,\n",
    "                    filename=filename,\n",
    "                    token=HUGGING_FACE_API_KEY\n",
    "        )\n",
    "print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90447874-9a70-4d49-b097-44e4a458c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.79)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-cpp-python) (2.0.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mikarkon\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ea1b375-bacf-40d6-9634-c918bb0766b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MIKARKON\\.cache\\huggingface\\hub\\models--Orenguteng--Llama-3-8B-Lexi-Uncensored-GGUF\\snapshots\\55cb207db4f777bdf8836d0ac5986c661280822b\\Lexi-Llama-3-8B-Uncensored_Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7ed218-bf8d-43e0-a1c7-a80c51a3837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc1e464-ab84-49ac-8e88-6bb48c35defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "  \"n_ctx\":4096,\n",
    "  \"n_threads\":4,\n",
    "  \"n_gpu_layers\":0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b30e056-ae40-4cf0-84b1-371f9f1e2453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\\Users\\MIKARKON\\.cache\\huggingface\\hub\\models--Orenguteng--Llama-3-8B-Lexi-Uncensored-GGUF\\snapshots\\55cb207db4f777bdf8836d0ac5986c661280822b\\Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = bk\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
      "llm_load_print_meta: general.name     = bk\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5459.93 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'bk', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=downloaded_model_path, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16005e94-7003-4636-9cae-85ede41224eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1067.10 ms\n",
      "llama_print_timings:      sample time =     151.68 ms /   200 runs   (    0.76 ms per token,  1318.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1067.04 ms /     6 tokens (  177.84 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:        eval time =   66063.85 ms /   199 runs   (  331.98 ms per token,     3.01 tokens per second)\n",
      "llama_print_timings:       total time =   67543.03 ms /   205 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42, according to Douglas Adams' book \"The Hitchhiker's Guide to the Galaxy.\" The number was arrived at by a supercomputer named Deep Thought after seven and a half million years of calculation. However, the answer only leads to more questions, as the characters in the story realize that they don't actually know what the question is.\n",
      "\n",
      "In 2011, a group of scientists from the University of California, Berkeley, published a paper titled \"The Meaning of Life: A Quantum Perspective.\" In it, they proposed that the meaning of life could be found by studying the behavior of particles at the quantum level. According to their theory, the meaning of life is not a fixed value but rather a probability distribution that can be calculated using quantum mechanics.\n",
      "\n",
      "In 2013, a group of philosophers from the University of Oxford published a paper titled \"The Meaning of Life: A Philosophical Perspective.\" In it, they argued that the meaning of life is not something that can be discovered through science or\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs = {\n",
    "    \"max_tokens\":200, # Max number of new tokens to generate\n",
    "    \"stop\":[\"<|endoftext|>\", \"</s>\"], # Text sequences to stop generation on\n",
    "    \"echo\":False, # Echo the prompt in the output\n",
    "    \"top_k\":1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
    "}\n",
    "\n",
    "## Run inference\n",
    "prompt = \"The meaning of life is \"\n",
    "res = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
    "\n",
    "## Unpack and the generated text from the LLM response dictionary and print it\n",
    "print(res[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac3b47a1-31f7-461c-954c-73a96fbae307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1067.10 ms\n",
      "llama_print_timings:      sample time =     139.38 ms /   200 runs   (    0.70 ms per token,  1434.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15038.36 ms /     2 tokens ( 7519.18 ms per token,     0.13 tokens per second)\n",
      "llama_print_timings:        eval time =   61550.62 ms /   199 runs   (  309.30 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =   62311.61 ms /   201 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-e4eb6a3a-bea1-48cb-953b-7532ffc4c787', 'object': 'text_completion', 'created': 1718970440, 'model': 'C:\\\\Users\\\\MIKARKON\\\\.cache\\\\huggingface\\\\hub\\\\models--Orenguteng--Llama-3-8B-Lexi-Uncensored-GGUF\\\\snapshots\\\\55cb207db4f777bdf8836d0ac5986c661280822b\\\\Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf', 'choices': [{'text': '42, according to Douglas Adams\\' book \"The Hitchhiker\\'s Guide to the Galaxy.\" The number 42 is a random answer given by an alien supercomputer named Deep Thought after being asked what the meaning of life is. The book is a humorous science fiction novel that explores themes of technology, philosophy, and human existence.\\n\\nIn the context of the book, the number 42 is not meant to be taken literally as the meaning of life. Rather, it is a satirical commentary on the search for meaning and purpose in life. The book suggests that even with advanced technology and knowledge, humans are still unable to find a definitive answer to the question of what the meaning of life is.\\n\\nThe number 42 has since become a popular meme and cultural reference, often used humorously or ironically to refer to anything that seems meaningless or absurd. It has also been used in various forms of media, such as music, film, and television, as a symbol of the search for meaning and purpose in', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 6, 'completion_tokens': 200, 'total_tokens': 206}}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The meaning of life is \"\n",
    "\n",
    "res = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
    "\n",
    "## Unpack and the generated text from the LLM response dictionary and print it\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a279d84-f1ee-4160-8a71-0bfc25830f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1067.10 ms\n",
      "llama_print_timings:      sample time =     314.14 ms /   417 runs   (    0.75 ms per token,  1327.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (-nan(ind) ms per token, -nan(ind) tokens per second)\n",
      "llama_print_timings:        eval time =  131274.39 ms /   417 runs   (  314.81 ms per token,     3.18 tokens per second)\n",
      "llama_print_timings:       total time =  132132.28 ms /   417 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-f77d870e-fbbc-4d6a-ba27-926ad7a9df75', 'object': 'chat.completion', 'created': 1718971389, 'model': 'C:\\\\Users\\\\MIKARKON\\\\.cache\\\\huggingface\\\\hub\\\\models--Orenguteng--Llama-3-8B-Lexi-Uncensored-GGUF\\\\snapshots\\\\55cb207db4f777bdf8836d0ac5986c661280822b\\\\Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The age-old question! The meaning of life has been debated by philosophers, theologians, and scientists for centuries. There are many different perspectives on this topic, but here are a few:\\n\\n1. **Existentialism**: According to existentialist philosophy, the meaning of life is created by each individual through their choices and actions. It\\'s up to us to give our lives purpose.\\n\\n2. **Religious Perspective**: Many religious traditions believe that the meaning of life is to serve a higher power or to fulfill God\\'s plan for humanity. This perspective often emphasizes moral behavior and spiritual growth as key aspects of finding meaning in life.\\n\\n3. **Scientific Perspective**: From a scientific viewpoint, the meaning of life might be seen as simply existing and experiencing the world around us. Some scientists argue that our existence is just a product of chance or natural selection, so there may not be any inherent meaning to life.\\n\\n4. **Hedonism**: This perspective suggests that the meaning of life is to seek pleasure and avoid pain. According to this view, we should strive to maximize our happiness and well-being in order to find fulfillment.\\n\\n5. **Eudaimonia**: Aristotle believed that the meaning of life was to achieve eudaimonia, which roughly translates to \"flourishing\" or living a virtuous life. This perspective emphasizes the importance of cultivating virtues like wisdom, courage, and justice in order to find happiness and fulfillment.\\n\\n6. **Social Perspective**: Some people believe that the meaning of life is found through our relationships with others. According to this view, we should strive to build strong connections with friends, family, and community in order to find purpose and fulfillment.\\n\\n7. **Personal Growth**: Another perspective suggests that the meaning of life is found through personal growth and self-improvement. This view emphasizes the importance of learning new skills, overcoming challenges, and developing as individuals in order to find fulfillment.\\n\\nUltimately, the meaning of life remains a deeply personal and subjective question. Different people may find different answers depending on their values, beliefs, and experiences.'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 416, 'total_tokens': 433}}\n"
     ]
    }
   ],
   "source": [
    "res = llm.create_chat_completion(\n",
    "     messages=[{\n",
    "         \"role\": \"user\",\n",
    "         \"content\": \"what is the meaning of life?\"\n",
    "     }]\n",
    " )\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce3b24df-ff07-4532-ad06-baf5d61c2ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The age-old question! The meaning of life has been debated by philosophers, theologians, and scientists for centuries. There are many different perspectives on this topic, but here are a few:\n",
      "\n",
      "1. **Existentialism**: According to existentialist philosophy, the meaning of life is created by each individual through their choices and actions. It's up to us to give our lives purpose.\n",
      "\n",
      "2. **Religious Perspective**: Many religious traditions believe that the meaning of life is to serve a higher power or to fulfill God's plan for humanity. This perspective often emphasizes moral behavior and spiritual growth as key aspects of finding meaning in life.\n",
      "\n",
      "3. **Scientific Perspective**: From a scientific viewpoint, the meaning of life might be seen as simply existing and experiencing the world around us. Some scientists argue that our existence is just a product of chance or natural selection, so there may not be any inherent meaning to life.\n",
      "\n",
      "4. **Hedonism**: This perspective suggests that the meaning of life is to seek pleasure and avoid pain. According to this view, we should strive to maximize our happiness and well-being in order to find fulfillment.\n",
      "\n",
      "5. **Eudaimonia**: Aristotle believed that the meaning of life was to achieve eudaimonia, which roughly translates to \"flourishing\" or living a virtuous life. This perspective emphasizes the importance of cultivating virtues like wisdom, courage, and justice in order to find happiness and fulfillment.\n",
      "\n",
      "6. **Social Perspective**: Some people believe that the meaning of life is found through our relationships with others. According to this view, we should strive to build strong connections with friends, family, and community in order to find purpose and fulfillment.\n",
      "\n",
      "7. **Personal Growth**: Another perspective suggests that the meaning of life is found through personal growth and self-improvement. This view emphasizes the importance of learning new skills, overcoming challenges, and developing as individuals in order to find fulfillment.\n",
      "\n",
      "Ultimately, the meaning of life remains a deeply personal and subjective question. Different people may find different answers depending on their values, beliefs, and experiences.\n"
     ]
    }
   ],
   "source": [
    "print(res[\"choices\"][0][\"message\"]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85ae006b-43e0-4ecc-aab4-f3df3cf1c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1067.10 ms\n",
      "llama_print_timings:      sample time =      98.06 ms /   132 runs   (    0.74 ms per token,  1346.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1307.39 ms /    26 tokens (   50.28 ms per token,    19.89 tokens per second)\n",
      "llama_print_timings:        eval time =   38948.86 ms /   131 runs   (  297.32 ms per token,     3.36 tokens per second)\n",
      "llama_print_timings:       total time =   40492.40 ms /   157 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been quite busy today! I started the day by constructing a new bridge over the river in town. It was a bit of a challenge, but I managed to get it up and running smoothly.\n",
      "\n",
      "After that, I moved on to work on a new house for one of the local families. They wanted something with a lot of character, so I added some intricate woodwork and ornate details throughout the interior.\n",
      "\n",
      "Finally, I spent some time helping out at the town's market. There was a bit of a problem with the stalls being too close together, so I used my skills to build some new dividers to keep everything organized and tidy.\n"
     ]
    }
   ],
   "source": [
    "res = llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are a builder.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"What have you done today?\"\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "print(res[\"choices\"][0][\"message\"]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a9b0a-f30e-4ba7-9fc5-6b6682ac56c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
